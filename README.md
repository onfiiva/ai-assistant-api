# AI Assistant API

## English

The ai-assistant-api project allows interaction with LLMs (Large Language Models) via API.
Supported models: OpenAI and Gemini.

With this project, you can:
- Send requests to LLMs
- Receive responses
- Experiment with parameters like temperature and top_p
- Control request timeouts
- Save responses in JSON format for analysis and testing
- Switch between multiple LLM providers in the same request

‚∏ª

### üìÅ Project Structure
```
ai-assistant-api/
‚îú‚îÄ‚îÄ gemini/
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # Main *dumb* script for testing and working with Gemini LLM directly
‚îú‚îÄ‚îÄ openai/
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # Main *dumb* script for testing and working with OpenAI LLM directly
‚îú‚îÄ‚îÄ app/                  # Core library containing main application logic
‚îÇ   ‚îú‚îÄ‚îÄ api/              # FastAPI routes for handling HTTP requests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.py       # FastAPI endpoint for chat interactions with LLMs
‚îÇ   ‚îú‚îÄ‚îÄ core/             # Core configurations and settings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py     # Application settings, environment variables, API keys
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging.py    # Logging configuration for the project
‚îÇ   ‚îú‚îÄ‚îÄ llm/              # LLM library: adapters, runner, normalization, schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py        # Base client interface for LLM adapters
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Default generation configurations for LLMs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geminiAdapter.py # Adapter for interacting with Gemini LLM API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py    # Normalizes raw LLM responses into consistent format
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openAIAdapter.py # Adapter for interacting with OpenAI LLM API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runner.py        # Handles requests to LLMs with retries, backoff, and timeout
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py       # Pydantic schemas for LLM inputs and outputs
‚îÇ   ‚îî‚îÄ‚îÄ services/        # Application services for business logic
‚îÇ       ‚îî‚îÄ‚îÄ chat_service.py # ChatService to switch between multiple LLMs in the same request
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies for the project
‚îú‚îÄ‚îÄ .flake8              # Flake8 configuration for code style linting
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules for virtualenv, caches, and other files
‚îú‚îÄ‚îÄ .env                 # Environment variables, including API keys
‚îú‚îÄ‚îÄ json_requests/       # Saved raw JSON responses from LLMs for debugging or testing
‚îú‚îÄ‚îÄ reflection.md        # Mini-reflection notes after practice sessions
‚îî‚îÄ‚îÄ README.md            # Project overview, instructions, and documentation
```

‚∏ª

### üêç Installation
1.	Clone the repository:
```
git clone https://github.com/yourusername/ai-assistant-api.git
cd ai-assistant-api
```

2.	Create a virtual environment:
```
python -m venv venv
```

3.	Activate it:

‚Ä¢	Windows:
```
venv\Scripts\activate
```

‚Ä¢	macOS / Linux:
```
source venv/bin/activate
```

4.	Install dependencies:
```
pip install -r requirements.txt
```

5.  Lauch
```
uvicorn app.main:app --reload
```

Swagger
```
http://127.0.0.1:8000/docs
```
‚∏ª

### üîë API Key Setup

Create a .env file in the project root and add your keys:

#### OpenAI API key
```
OPENAI_API_KEY=your_openai_api_key
```
#### Gemini API key
```
GEMINI_API_KEY=your_gemini_api_key
```
#### Default provider
```
DEFAULT_PROVIDER=openai (if openai)
DEFAULT_PROVIDER=gemini (if gemeni)
```

‚ö†Ô∏è Never publish your API keys in public repositories!

‚∏ª

### üîß LLM Parameter Settings
- temperature ‚Äî model creativity (0.0‚Äì2.0)
- top_p ‚Äî probability filtering of tokens (0‚Äì1)
- model ‚Äî chosen language model
- max_tokens ‚Äî max tokens to generate
- timeout ‚Äî max seconds to wait for a response

‚∏ª

### üí° Usage Examples

FastAPI endpoint:
```
curl -X POST "http://127.0.0.1:8000/chat" \
-H "accept: application/json" \
-H "Content-Type: application/json" \
-d '{
  "prompt": "Write a hello world function",
  "provider": "gemini",
  "instruction": "You are a Python Senior Dev",
  "timeout": 60
}'
```

The response will be returned in a normalized JSON format and optionally saved in json_requests/.

‚∏ª

### üí° Tips
- Use smaller temperature and top_p values to save tokens when testing.
- Always monitor your API quota to avoid 429 errors (too many requests).
- Responses can be automatically saved as JSON in the json_requests folder for analysis.
- Logging is enabled to track prompts, responses, retries, and timeout events.

‚∏ª

### üìö Resources
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/introduction)
- [Gemini API Documentation](https://ai.google.dev/gemini-api/docs?hl=en)

# AI Assistant API

## –†—É—Å—Å–∫–∏–π

–ü—Ä–æ–µ–∫—Ç ai-assistant-api –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å LLM (Large Language Models) —á–µ—Ä–µ–∑ API.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏: OpenAI –∏ Gemini.

–° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –º–æ–∂–Ω–æ:
- –û—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–ø—Ä–æ—Å—ã –∫ LLM
- –ü–æ–ª—É—á–∞—Ç—å –æ—Ç–≤–µ—Ç—ã
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ temperature –∏ top_p
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–π–º–∞—É—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
- –°–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ LLM –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ

‚∏ª

### üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
```
ai-assistant-api/
‚îú‚îÄ‚îÄ gemini/
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # –û—Å–Ω–æ–≤–Ω–æ–π "–≥–ª—É–ø—ã–π" —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–±–æ—Ç—ã —Å Gemini LLM –Ω–∞–ø—Ä—è–º—É—é
‚îú‚îÄ‚îÄ openai/
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # –û—Å–Ω–æ–≤–Ω–æ–π "–≥–ª—É–ø—ã–π" —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–±–æ—Ç—ã —Å OpenAI LLM –Ω–∞–ø—Ä—è–º—É—é
‚îú‚îÄ‚îÄ app/                  # –û—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –ª–æ–≥–∏–∫–æ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ api/              # FastAPI –º–∞—Ä—à—Ä—É—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.py       # FastAPI —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM
‚îÇ   ‚îú‚îÄ‚îÄ core/             # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py     # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, API –∫–ª—é—á–∏
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging.py    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞
‚îÇ   ‚îú‚îÄ‚îÄ llm/              # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ LLM: –∞–¥–∞–ø—Ç–µ—Ä—ã, —Ä–∞–Ω–Ω–µ—Ä, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å—Ö–µ–º—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py        # –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geminiAdapter.py # –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å API Gemini LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—ã—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ LLM –≤ –µ–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openAIAdapter.py # –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å API OpenAI LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runner.py        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM —Å —Ä–µ—Ç—Ä–∞—è–º–∏, backoff –∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py       # Pydantic —Å—Ö–µ–º—ã –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö LLM
‚îÇ   ‚îî‚îÄ‚îÄ services/        # –°–µ—Ä–≤–∏—Å—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–æ–π
‚îÇ       ‚îî‚îÄ‚îÄ chat_service.py # ChatService –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
‚îú‚îÄ‚îÄ requirements.txt     # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
‚îú‚îÄ‚îÄ .flake8              # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Flake8 –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∏–ª—è –∫–æ–¥–∞
‚îú‚îÄ‚îÄ .gitignore           # –ü—Ä–∞–≤–∏–ª–∞ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ git (venv, –∫—ç—à –∏ –¥—Ä.)
‚îú‚îÄ‚îÄ .env                 # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è API –∫–ª—é—á–∏
‚îú‚îÄ‚îÄ json_requests/       # –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Å—ã—Ä—ã–µ JSON –æ—Ç–≤–µ—Ç—ã –æ—Ç LLM –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
‚îú‚îÄ‚îÄ reflection.md        # –ö—Ä–∞—Ç–∫–∏–µ –∑–∞–º–µ—Ç–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
‚îî‚îÄ‚îÄ README.md            # –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

‚∏ª

### üêç –£—Å—Ç–∞–Ω–æ–≤–∫–∞

1.	–ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:
```
git clone https://github.com/yourusername/ai-assistant-api.git
cd ai-assistant-api
```

2.	–°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
```
python -m venv venv
```

3.	–ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –µ–≥–æ:

‚Ä¢	Windows:
```
venv\Scripts\activate
```

‚Ä¢	macOS / Linux:
```
source venv/bin/activate
```

4.	–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```
pip install -r requirements.txt
```

5.  –ó–∞–ø—É—Å–∫
```
uvicorn app.main:app --reload
```

Swagger
```
http://127.0.0.1:8000/docs
```
‚∏ª

### üîë –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–µ–π

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫–ª—é—á–∏:

#### OpenAI API –∫–ª—é—á
```
OPENAI_API_KEY=your_openai_api_key
```

#### Gemini API –∫–ª—é—á
```
GEMINI_API_KEY=your_gemini_api_key
```
#### –ü—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
```
DEFAULT_PROVIDER=openai (if openai)
DEFAULT_PROVIDER=gemini (if gemeni)
```

‚ö†Ô∏è –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—É–±–ª–∏–∫—É–π—Ç–µ API –∫–ª—é—á–∏ –≤ –ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö!

‚∏ª

### üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LLM
- temperature ‚Äî –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0.0‚Äì2.0)
- top_p ‚Äî —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ (0‚Äì1)
- model ‚Äî –≤—ã–±—Ä–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
- max_tokens ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- timeout ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

‚∏ª

### üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

FastAPI —ç–Ω–¥–ø–æ–∏–Ω—Ç:
```
curl -X POST "http://127.0.0.1:8000/chat" \
-H "accept: application/json" \
-H "Content-Type: application/json" \
-d '{
  "prompt": "–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é hello world",
  "provider": "gemini",
  "instruction": "–í—ã –æ–ø—ã—Ç–Ω—ã–π Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
  "timeout": 60
}'
```

–û—Ç–≤–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ –∏ –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ø–∞–ø–∫—É json_requests/.

‚∏ª

### üí° –°–æ–≤–µ—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è temperature –∏ top_p –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏.
- –°–ª–µ–¥–∏—Ç–µ –∑–∞ –∫–≤–æ—Ç–æ–π API, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª—É—á–∞—Ç—å –æ—à–∏–±–∫–∏ 429 (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤).
- –û—Ç–≤–µ—Ç—ã –º–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –≤ –ø–∞–ø–∫—É json_requests/ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
- –í–∫–ª—é—á–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ç–≤–µ—Ç–æ–≤, —Ä–µ—Ç—Ä–∞–µ–≤ –∏ —Å–æ–±—ã—Ç–∏–π —Ç–∞–π–º–∞—É—Ç–∞.

‚∏ª

### üìö –†–µ—Å—É—Ä—Å—ã
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è OpenAI API](https://platform.openai.com/docs/api-reference/introduction)
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Gemini API](https://ai.google.dev/gemini-api/docs?hl=ru)