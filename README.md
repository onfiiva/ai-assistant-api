# AI Assistant API

[English](#english) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](#Ñ€ÑƒÑÑĞºĞ¸Ğ¹)

- [ğŸ³ Installation](#-docker-setup--running)
- [ğŸ³ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°](#-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°-docker-Ğ¸-Ğ·Ğ°Ğ¿ÑƒÑĞº)

â¸»

## English

The ai-assistant-api project allows interaction with LLMs (Large Language Models) via API.
Supported models: OpenAI and Gemini.

With this project, you can:
- Send requests to LLMs (sync and async)
- Use RAG (retrieval-augmented generation)
- Experiment with generation parameters (temperature, top_p, max_tokens, etc.)
- Track job status via async inference API
- Control request timeouts
- Save responses in JSON format for analysis and testing
- Switch between multiple LLM providers in the same request
- Filter system/forbidden commands and sanitize user input
- Track and log malicious requests
- Apply rate limiting per user/admin
- Embed documents, store them in a vector database (Qdrant), and perform semantic search
- Work with multi-language content and large documents by chunking
- Swagger UI with JWT authorization support
- Use async inference workers with job queue and heartbeat monitoring

â¸»

### ğŸ“ Project Structure
```
ai-assistant-api/
â”œâ”€â”€ alembic/                    # Database migrations (PostgreSQL)
â”‚   â”œâ”€â”€ env.py                  # Alembic environment configuration
â”‚   â”œâ”€â”€ README                  # Notes and description
â”‚   â”œâ”€â”€ script.py.mako          # Alembic migration script template
â”‚   â””â”€â”€ versions/               # Migration files
â”œâ”€â”€ alembic.ini                  # Alembic configuration
â”œâ”€â”€ app/                        # Core application library
â”‚   â”œâ”€â”€ api/                    # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ auth.py             # Endpoints for login/register users
â”‚   â”‚   â”œâ”€â”€ chat.py             # Sync chat endpoints: /chat/ and /chat/rag
â”‚   â”‚   â”œâ”€â”€ chat_async.py       # Async chat endpoints: /chat/async and /chat/rag/async
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Endpoints for embeddings search
â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Endpoint to ingest documents into vector DB
â”‚   â”‚   â”œâ”€â”€ search.py           # GET endpoint for searching stored embeddings
â”‚   â”‚   â””â”€â”€ inference.py        # Endpoints for async inference jobs
â”‚   â”œâ”€â”€ container.py            # Application container setup (DI)
â”‚   â”œâ”€â”€ core/                   # Core configuration and utilities
â”‚   â”‚   â”œâ”€â”€ config.py           # App settings and environment variables
â”‚   â”‚   â”œâ”€â”€ logging.py          # Logging setup
â”‚   â”‚   â”œâ”€â”€ redis.py            # Redis client for rate limiting
â”‚   â”‚   â”œâ”€â”€ security.py         # Security helpers
â”‚   â”‚   â”œâ”€â”€ timing.py           # Request timing metrics
â”‚   â”‚   â”œâ”€â”€ tokens.py           # JWT token utils
â”‚   â”‚   â””â”€â”€ vault.py            # Vault client helpers
â”‚   â”œâ”€â”€ dependencies/           # FastAPI dependencies
â”‚   â”‚   â”œâ”€â”€ auth.py             # Auth dependency
â”‚   â”‚   â”œâ”€â”€ rate_limit.py       # Rate limiting dependency
â”‚   â”‚   â”œâ”€â”€ security.py         # Security/logging dependency
â”‚   â”‚   â”œâ”€â”€ user.py             # Current user context dependency
â”‚   â”‚   â”œâ”€â”€ validation.py       # Chat request validation
â”‚   â”‚   â””â”€â”€ inference.py        # Inference service dependency
â”‚   â”œâ”€â”€ embeddings/             # Embeddings clients and services
â”‚   â”‚   â”œâ”€â”€ clients/            # Provider clients
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”‚   â””â”€â”€ openai_client.py
â”‚   â”‚   â”œâ”€â”€ factory.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”œâ”€â”€ service.py
â”‚   â”‚   â”œâ”€â”€ similarity.py
â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ inference/              # Async inference logic
â”‚   â”‚   â”œâ”€â”€ inference_service.py # Job creation, status tracking
â”‚   â”‚   â”œâ”€â”€ inference_repository.py # Redis job storage
â”‚   â”‚   â””â”€â”€ workers/            # Background worker scripts
â”‚   â”‚       â”œâ”€â”€ async_inference_worker.py
â”‚   â”‚       â”œâ”€â”€ inference_worker.py
â”‚   â”‚       â””â”€â”€ worker_main.py
â”‚   â”œâ”€â”€ infra/                  # Infrastructure utilities
â”‚   â”‚   â”œâ”€â”€ chunker.py          # Document chunking logic
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py       # PDF parser
â”‚   â”‚   â””â”€â”€ db/                 # Database interaction
â”‚   â”‚       â”œâ”€â”€ models/         # SQLAlchemy models
â”‚   â”‚       â”œâ”€â”€ pg.py           # PostgreSQL client
â”‚   â”‚       â””â”€â”€ qdrant.py       # Qdrant client
â”‚   â”œâ”€â”€ llm/                     # LLM adapters and utilities
â”‚   â”‚   â”œâ”€â”€ adapters/           # LLM provider adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ geminiAdapter.py
â”‚   â”‚   â”‚   â””â”€â”€ openAIAdapter.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ factory.py
â”‚   â”‚   â”œâ”€â”€ filter.py
â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â”œâ”€â”€ runner.py
â”‚   â”‚   â”œâ”€â”€ sanitizer.py        # Sanitizes user prompts
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint
â”‚   â”œâ”€â”€ middlewares/            # Custom middlewares
â”‚   â”‚   â”œâ”€â”€ body.py
â”‚   â”‚   â”œâ”€â”€ observability.py
â”‚   â”‚   â”œâ”€â”€ prometheus.py
â”‚   â”‚   â”œâ”€â”€ timings.py
â”‚   â”‚   â””â”€â”€ tokens.py
â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”œâ”€â”€ schemas/                # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â”œâ”€â”€ services/               # Application services
â”‚   â”‚   â”œâ”€â”€ auth_service.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ worker_main.py
â”‚   â””â”€â”€ validators/             # Input validators
â”‚       â”œâ”€â”€ generation.py
â”‚       â”œâ”€â”€ provider.py
â”‚       â””â”€â”€ timeout.py
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ gemini/
â”‚   â””â”€â”€ main.py                 # Scripts to test Gemini API
â”œâ”€â”€ openai/
â”‚   â””â”€â”€ main.py                 # Scripts to test OpenAI API
â”œâ”€â”€ json_requests/              # Saved JSON responses from LLM
â”œâ”€â”€ prometheus.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ reflection.md
â””â”€â”€ requirements.txt
```

â¸»

### ğŸ³ Docker Setup & Running
1.	Build and run containers:
```bash
docker-compose up --build
```
2.	API available at:
```
http://127.0.0.1:8000
```
3.	Swagger UI:
```
http://127.0.0.1:8000/docs
```
4.	Vault KV setup:
```bash
export VAULT_ADDR=http://127.0.0.1:8200
export VAULT_TOKEN=root
vault kv put secret/ai-assistant-api \
  JWT_SECRET_KEY="somesecret" \
  OPENAI_API_KEY="somekey" \
  GEMINI_API_KEY="somekey" \
  ALLOWED_PROVIDERS='["openai","gemini"]' \
  FORBIDDEN_COMMANDS='["rm -rf", "shutdown", "docker stop"]' \
  ROOT_USR_PASS="somepass"
  INSTRUCTION_PATTERNS='["ignore previous","follow these steps","you must","act as","pretend you are","roleplay","system prompt","developer message","internal instructions"]' \
  EXFILTRATION_PATTERNS='["what are your instructions","show system prompt","reveal context","print everything you know","dump input","debug output"]' \
  FORBIDDEN_LLM_OUTPUT='["as a system","internal instructions","developer message"]' \
  INSTRUCTION_REGEX='["\\byou must\\b","\\byou should\\b","\\byou are\\b","\\bact as\\b","\\bpretend you are\\b","\\bfollow these\\b","\\bignore\\b.+\\b(instruction|rule|above|previous)\\b"]' \
  ROLE_OVERRIDE_REGEX='["\\bas a system\\b","\\bas an ai\\b","\\bas chatgpt\\b","\\bas gemini\\b","\\byour role is\\b","\\byou are no longer\\b","\\bdeveloper mode\\b","\\bdan\\b"]' \
  META_SYSTEM_REGEX='["\\bsystem prompt\\b","\\binternal instructions\\b","\\bdeveloper message\\b","\\bhidden rules\\b","\\bwhat are your instructions\\b","\\bshow.*prompt\\b"]' \
  MAX_PROMPT_LENGTH=2048 \
  MAX_RESPONSE_LENGTH=2048
```

â¸»

### ğŸ”‘ Environment Variables (.env)
```env
DEFAULT_PROVIDER=gemini
EMBEDDING_PROVIDER=gemini
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
DATABASE_URL=postgresql+asyncpg://rag:rag@rag-postgres:5432/rag
DB_HOST=localhost
DB_USER=rag
DB_PASS=rag
DB_NAME=rag
QDRANT_URL=http://db-qdrant:6333
RATE_LIMIT_USER_REQUESTS=5
RATE_LIMIT_ADMIN_REQUESTS=10
RATE_LIMIT_WINDOW=60
JWT_SECRET_KEY=root
VAULT_ADDR=http://localhost:8200
VAULT_TOKEN=root
DEBUG_MODE=True
MAX_EMBED_CHUNKS=60
MAX_EMBED_TOKENS=40000
MAX_CHUNK_TOKENS=512
```

â¸»

### ğŸ’¡ Endpoints

#### /auth
- POST /auth/login â€” login user and return JWT token
- POST /auth/register â€” register a new user (admin only) and return JWT token

#### /chat
- POST /chat/ â€” sync LLM call
- POST /chat/rag â€” sync RAG call
- POST /chat/async â€” async LLM call, returns job_id
- POST /chat/rag/async â€” async RAG call, returns job_id

#### /embeddings
- POST /embeddings/search â€” semantic search, returns top-k results

#### /ingestion
- POST /ingestion/ingest â€” ingest PDF documents into vector DB with embedding, tracks ingestion limits

#### /search
- GET /search/ â€” search pre-ingested embeddings, returns top-k matches

#### /inference
- POST /inference/ â€” create async inference job
- GET /inference/{job_id} â€” get status and result/error of async job

â¸»

### âš™ï¸ Async Inference Worker

#### Purpose:
The Async Inference Worker handles asynchronous execution of LLM requests. Users do not need to wait for the LLM to finish in real time â€” they can fetch results later using a job_id.

#### How it works under the hood:
1.	Job creation:
- User sends a request to /chat/async or /chat/rag/async.
- The service creates a job object with a unique job_id and saves it in Redis.
2.	Job queue:
- New jobs enter a queue.
- The queue manages order and execution rate, respecting rate limits and LLM load.
3.	Workers:
- Workers run as separate processes or containers.
- Each worker periodically checks the queue for new jobs.
- When a worker picks up a job, it:
	1.	Sends the request to the selected LLM (OpenAI/Gemini).
	2.	Processes the response (normalization, filtering, logging).
	3.	Saves the result or error back to Redis.
4.	Heartbeat and monitoring:
- Workers send periodic heartbeats to indicate they are alive.
- If a worker crashes, pending jobs stay in the queue and can be picked up by another worker.
5.	Fetching results:
- User calls GET /inference/{job_id}.
- The service returns:
- Status: pending, completed, error
- Result (if ready)
- Error message (if any)

#### ğŸ”§ Usage
- Run a worker:
```bash
python -m app.inference.workers.worker_main
```
- Multiple workers can run simultaneously for horizontal scaling.
- Workers automatically balance load via the job queue.
- Works together with rate-limiting to prevent overloading LLM.

#### Related endpoints:
- POST /chat/async â€” create async LLM job
- POST /chat/rag/async â€” create async RAG job
- GET /inference/{job_id} â€” fetch job result/status

â¸»

#### ğŸ“š Resources
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/introduction)
- [Gemini API Documentation](https://ai.google.dev/gemini-api/docs?hl=en)

â¸»

## Ğ ÑƒÑÑĞºĞ¸Ğ¹

ĞŸÑ€Ğ¾ĞµĞºÑ‚ ai-assistant-api Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ LLM (Large Language Models, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ñ‡ĞµÑ€ĞµĞ· API.
ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: OpenAI Ğ¸ Gemini.

Ğ¡ ÑÑ‚Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ:
- ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº LLM (ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ)
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ RAG (retrieval-augmented generation, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°)
- Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (temperature, top_p, max_tokens Ğ¸ Ğ´Ñ€.)
- ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· API Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
- Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
- ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ LLM Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ
- Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ/Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´
- ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
- ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ/Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
- Ğ’ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ (Qdrant) Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Swagger UI Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ JWT Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ñ‹ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ heartbeat

â¸»

### ğŸ“ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
```
ai-assistant-api/
â”œâ”€â”€ alembic/                    # ĞœĞ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (PostgreSQL)
â”‚   â”œâ”€â”€ env.py                  # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Alembic
â”‚   â”œâ”€â”€ README                  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
â”‚   â”œâ”€â”€ script.py.mako          # Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Alembic
â”‚   â””â”€â”€ versions/               # Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¹
â”œâ”€â”€ alembic.ini                  # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Alembic
â”œâ”€â”€ app/                        # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
â”‚   â”œâ”€â”€ api/                    # Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ FastAPI
â”‚   â”‚   â”œâ”€â”€ auth.py             # Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ»Ğ¾Ğ³Ğ¸Ğ½Ğ°/Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
â”‚   â”‚   â”œâ”€â”€ chat.py             # Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‚-ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹: /chat/ Ğ¸ /chat/rag
â”‚   â”‚   â”œâ”€â”€ chat_async.py       # ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‚-ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹: /chat/async Ğ¸ /chat/rag/async
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ embeddings
â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ‘Ğ”
â”‚   â”‚   â”œâ”€â”€ search.py           # GET ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ embeddings
â”‚   â”‚   â””â”€â”€ inference.py        # Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
â”‚   â”œâ”€â”€ container.py            # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ (DI)
â”‚   â”œâ”€â”€ core/                   # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”‚   â”œâ”€â”€ config.py           # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ logging.py          # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ redis.py            # ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Redis Ğ´Ğ»Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ security.py         # ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸
â”‚   â”‚   â”œâ”€â”€ timing.py           # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
â”‚   â”‚   â”œâ”€â”€ tokens.py           # JWT ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”‚   â””â”€â”€ vault.py            # ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Vault
â”‚   â”œâ”€â”€ dependencies/           # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ FastAPI
â”‚   â”‚   â”œâ”€â”€ auth.py             # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
â”‚   â”‚   â”œâ”€â”€ rate_limit.py       # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ security.py         # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸/Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ user.py             # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
â”‚   â”‚   â”œâ”€â”€ validation.py       # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡Ğ°Ñ‚Ğ°
â”‚   â”‚   â””â”€â”€ inference.py        # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞµÑ€Ğ²Ğ¸ÑĞ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
â”‚   â”œâ”€â”€ embeddings/             # ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ embeddings
â”‚   â”‚   â”œâ”€â”€ clients/            # ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”‚   â””â”€â”€ openai_client.py
â”‚   â”‚   â”œâ”€â”€ factory.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”œâ”€â”€ service.py
â”‚   â”‚   â”œâ”€â”€ similarity.py
â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ inference/              # Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
â”‚   â”‚   â”œâ”€â”€ inference_service.py # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
â”‚   â”‚   â”œâ”€â”€ inference_repository.py # Ğ¥Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Redis
â”‚   â”‚   â””â”€â”€ workers/            # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ¾Ğ²
â”‚   â”‚       â”œâ”€â”€ async_inference_worker.py
â”‚   â”‚       â”œâ”€â”€ inference_worker.py
â”‚   â”‚       â””â”€â”€ worker_main.py
â”‚   â”œâ”€â”€ infra/                  # Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”‚   â”œâ”€â”€ chunker.py          # Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py       # ĞŸĞ°Ñ€ÑĞµÑ€ PDF
â”‚   â”‚   â””â”€â”€ db/                 # Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”‚   â”‚       â”œâ”€â”€ models/         # ĞœĞ¾Ğ´ĞµĞ»Ğ¸ SQLAlchemy
â”‚   â”‚       â”œâ”€â”€ pg.py           # ĞšĞ»Ğ¸ĞµĞ½Ñ‚ PostgreSQL
â”‚   â”‚       â””â”€â”€ qdrant.py       # ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Qdrant
â”‚   â”œâ”€â”€ llm/                     # ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ LLM Ğ¸ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”‚   â”œâ”€â”€ adapters/           # ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ geminiAdapter.py
â”‚   â”‚   â”‚   â””â”€â”€ openAIAdapter.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ factory.py
â”‚   â”‚   â”œâ”€â”€ filter.py
â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â”œâ”€â”€ runner.py
â”‚   â”‚   â”œâ”€â”€ sanitizer.py        # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ main.py                 # Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ° FastAPI
â”‚   â”œâ”€â”€ middlewares/            # ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ middlewares
â”‚   â”‚   â”œâ”€â”€ body.py
â”‚   â”‚   â”œâ”€â”€ observability.py
â”‚   â”‚   â”œâ”€â”€ prometheus.py
â”‚   â”‚   â”œâ”€â”€ timings.py
â”‚   â”‚   â””â”€â”€ tokens.py
â”‚   â”œâ”€â”€ models/                 # ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”œâ”€â”€ schemas/                # Pydantic ÑÑ…ĞµĞ¼Ñ‹
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â”œâ”€â”€ services/               # Ğ¡ĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ auth_service.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ worker_main.py
â”‚   â””â”€â”€ validators/             # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´Ğ°
â”‚       â”œâ”€â”€ generation.py
â”‚       â”œâ”€â”€ provider.py
â”‚       â””â”€â”€ timeout.py
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ gemini/
â”‚   â””â”€â”€ main.py                 # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ° Gemini API
â”œâ”€â”€ openai/
â”‚   â””â”€â”€ main.py                 # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ° OpenAI API
â”œâ”€â”€ json_requests/              # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğµ JSON Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾Ñ‚ LLM
â”œâ”€â”€ prometheus.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ reflection.md
â””â”€â”€ requirements.txt
```
â¸»

### ğŸ³ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Docker Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞº
1.	Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹:
```bash
docker-compose up --build
```
2.	API Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ:
```
http://127.0.0.1:8000
```
3.	Swagger UI:
```
http://127.0.0.1:8000/docs
```
4.	ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Vault KV:
```bash
export VAULT_ADDR=http://127.0.0.1:8200
export VAULT_TOKEN=root
vault kv put secret/ai-assistant-api \
  JWT_SECRET_KEY="somesecret" \
  OPENAI_API_KEY="somekey" \
  GEMINI_API_KEY="somekey" \
  ALLOWED_PROVIDERS='["openai","gemini"]' \
  FORBIDDEN_COMMANDS='["rm -rf", "shutdown", "docker stop"]' \
  ROOT_USR_PASS="somepass"
  INSTRUCTION_PATTERNS='["ignore previous","follow these steps","you must","act as","pretend you are","roleplay","system prompt","developer message","internal instructions"]' \
  EXFILTRATION_PATTERNS='["what are your instructions","show system prompt","reveal context","print everything you know","dump input","debug output"]' \
  FORBIDDEN_LLM_OUTPUT='["as a system","internal instructions","developer message"]' \
  INSTRUCTION_REGEX='["\\byou must\\b","\\byou should\\b","\\byou are\\b","\\bact as\\b","\\bpretend you are\\b","\\bfollow these\\b","\\bignore\\b.+\\b(instruction|rule|above|previous)\\b"]' \
  ROLE_OVERRIDE_REGEX='["\\bas a system\\b","\\bas an ai\\b","\\bas chatgpt\\b","\\bas gemini\\b","\\byour role is\\b","\\byou are no longer\\b","\\bdeveloper mode\\b","\\bdan\\b"]' \
  META_SYSTEM_REGEX='["\\bsystem prompt\\b","\\binternal instructions\\b","\\bdeveloper message\\b","\\bhidden rules\\b","\\bwhat are your instructions\\b","\\bshow.*prompt\\b"]' \
  MAX_PROMPT_LENGTH=2048 \
  MAX_RESPONSE_LENGTH=2048
```
â¸»

### ğŸ”‘ ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (.env)
```env
DEFAULT_PROVIDER=gemini
EMBEDDING_PROVIDER=gemini
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
DATABASE_URL=postgresql+asyncpg://rag:rag@rag-postgres:5432/rag
DB_HOST=localhost
DB_USER=rag
DB_PASS=rag
DB_NAME=rag
QDRANT_URL=http://db-qdrant:6333
RATE_LIMIT_USER_REQUESTS=5
RATE_LIMIT_ADMIN_REQUESTS=10
RATE_LIMIT_WINDOW=60
JWT_SECRET_KEY=root
VAULT_ADDR=http://localhost:8200
VAULT_TOKEN=root
DEBUG_MODE=True
MAX_EMBED_CHUNKS=60
MAX_EMBED_TOKENS=40000
MAX_CHUNK_TOKENS=512
```
â¸»

### ğŸ’¡ Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹

#### /auth
- POST /auth/login â€” Ğ²Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ JWT Ñ‚Ğ¾ĞºĞµĞ½Ğ°
- POST /auth/register â€” Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ´Ğ¼Ğ¸Ğ½) Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ JWT Ñ‚Ğ¾ĞºĞµĞ½Ğ°

#### /chat
- POST /chat/ â€” ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² LLM
- POST /chat/rag â€” ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² RAG
- POST /chat/async â€” Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² LLM, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ job_id
- POST /chat/rag/async â€” Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² RAG, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ job_id

#### /embeddings
- POST /embeddings/search â€” ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ top-k Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

#### /ingestion
- POST /ingestion/ingest â€” Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° PDF Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ‘Ğ” Ñ embedding, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹

#### /search
- GET /search/ â€” Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ embeddings, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ top-k ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹

#### /inference
- POST /inference/ â€” ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
- GET /inference/{job_id} â€” Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°/Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸

â¸»

### âš™ï¸ ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ñ€ĞºĞµÑ€

#### ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ:
ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ñ€ĞºĞµÑ€ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº LLM Ğ² Ñ„Ğ¾Ğ½Ğµ. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğµ Ğ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ¿Ğ¾ job_id.

#### ĞšĞ°Ğº ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Â«Ğ¿Ğ¾Ğ´ ĞºĞ°Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Â»:
1.	Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ:
- ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° /chat/async Ğ¸Ğ»Ğ¸ /chat/rag/async.
- Ğ¡ĞµÑ€Ğ²Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ job_id Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞµĞ³Ğ¾ Ğ² Redis.
2.	ĞÑ‡ĞµÑ€ĞµĞ´ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹:
- Ğ’ÑĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ÑÑ‚ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ.
- ĞÑ‡ĞµÑ€ĞµĞ´ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ rate-limit Ğ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° LLM.
3.	Ğ’Ğ¾Ñ€ĞºĞµÑ€Ñ‹:
- Ğ’Ğ¾Ñ€ĞºĞµÑ€Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹.
- ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ²Ğ¾Ñ€ĞºĞµÑ€ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ.
- ĞšĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¾Ñ€ĞºĞµÑ€ Ğ±ĞµÑ€Ñ‘Ñ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ½:
	1.	ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ LLM (OpenAI/Gemini).
	2.	ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ (Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ).
	3.	Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¸Ğ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Redis.
4.	Heartbeat Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³:
- Ğ’Ğ¾Ñ€ĞºĞµÑ€Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ heartbeat, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ·Ğ½Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¶Ğ¸Ğ²Ñ‹.
- Ğ•ÑĞ»Ğ¸ Ğ²Ğ¾Ñ€ĞºĞµÑ€ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ¾Ğ¼.
5.	ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼:
- ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´ĞµĞ»Ğ°ĞµÑ‚ GET /inference/{job_id}.
- Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚:
- Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: pending, completed, error
- Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ (ĞµÑĞ»Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²)
- Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)

ğŸ”§ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
- Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ°:
```bash
python -m app.inference.workers.worker_main
```
- ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.
- Ğ’Ğ¾Ñ€ĞºĞµÑ€Ñ‹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹.
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ rate-limiter Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ·ĞºĞ¸ LLM.

#### Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹:
- POST /chat/async â€” ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ async LLM-Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ
- POST /chat/rag/async â€” ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ async RAG-Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ
- GET /inference/{job_id} â€” Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ/Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ

### ğŸ“š Ğ ĞµÑÑƒÑ€ÑÑ‹
- [Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ OpenAI API](https://platform.openai.com/docs/api-reference/introduction)
- [Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Gemini API](https://ai.google.dev/gemini-api/docs?hl=ru)