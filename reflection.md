# 1 занятие
## Что понял
- Что из себя в целом представляет LLM
- Как выглядят роли LLM моделей
- Какие параметры выставляются для генерации ответов от модели
- Как выглядят запросы
- Какие используются библиотеки для построения базовых запросов
- Что из себя представляют токены для моделей
- В чем может быть примерное отличие разных моделей друг от друга, и в чем их сходства

## Что было сложным
- Найти крупную модель с бесплатным Api Token :)

## Какие вопросы остались
- Как генерировать ответы на запросы (именно генерировать, а не подставлять строку ответа)
- Как выглядит потенциальное масштабирование
- Какие есть подводные камни в использовании тех или иных форм запросов
- Как строить запросы по генерации не только тексты, но и фото, видео и т.д.
- Есть ли возможность оптимизировать эти запросы

# 2 занятие
## Что понял
- Насколько различаются ответы в зависимости от temperature, top_p, контекста, ограничений
- Как примерно выстраивать модель LLM проекта для стандартизации обращений к разным API
- Как собирать основные данные с запроса к модели

## Что было сложным
- Если смотреть с точки зрения относительности, сложнее всего было организовать правильную структуру проекта с нормализацией и наследованием. В общем же, ничего не показалось чересчур сложным

## Какие вопросы остались
- Интересно, каким образом дальше можно использовать полученную информацию по промптингу в контексте LLM инженерии на примере действительных задач

# 3 занятие
## Что понял
- Как обернуть существующую LLM структуру в адекватный FastAPI
- Как в контексте LLM работать с timeout'ами, что логировать, как решать серверные ошибки и из-за чего они возникают

## Что было сложным
- Аккуратно внедрить в существующую структуру FastAPI, не затронув работающий функционал

## Какие вопросы остались
- Насколько глубже, чем написание LLM FastAPI, можно нырнуть